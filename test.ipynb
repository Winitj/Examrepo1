{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "\n",
    "# MySQL database connection settings\n",
    "db_settings = {\n",
    "    'host': 'localhost',\n",
    "    'port':'3307',                   # Use the IP or hostname of your MySQL container\n",
    "    'user': 'root',\n",
    "    'password': '',\n",
    "    'database': 'vinit',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_mapping = {\n",
    "    'B02512': 'Unter',\n",
    "    'B02598': 'Hinter',\n",
    "    'B02617': 'Weiter',\n",
    "    'B02682': 'Schmecken',\n",
    "    'B02764': 'Danach-NY',\n",
    "    'B02765': 'Grun',\n",
    "    'B02835': 'Dreist',\n",
    "    'B02836': 'Drinnen'\n",
    "}\n",
    "\n",
    "file_paths = [\n",
    "    \"C://Users//jain vinit//Documents//FExam//uber-tlc-foil-response//uber-trip-data//uber-raw-data-aug14.csv\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date/Time      Lat      Lon   Base\n",
      "0  8/1/2014 0:03:00  40.7366 -73.9906  Unter\n",
      "1  8/1/2014 0:09:00  40.7260 -73.9918  Unter\n",
      "2  8/1/2014 0:12:00  40.7209 -74.0507  Unter\n",
      "3  8/1/2014 0:12:00  40.7387 -73.9856  Unter\n",
      "4  8/1/2014 0:12:00  40.7323 -74.0077  Unter\n"
     ]
    }
   ],
   "source": [
    "ddf_list = [dd.read_csv(file) for file in file_paths]\n",
    "df = dd.concat(ddf_list)\n",
    "\n",
    "# Additional processing, mapping, etc.\n",
    "df['Base'] = df['Base'].map(base_mapping.get, na_action='ignore')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = mysql.connector.connect(**db_settings)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Create the table if it doesn't exist\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_data (\n",
    "    Date DATE,\n",
    "    Lat FLOAT,\n",
    "    Lon FLOAT,\n",
    "    Base VARCHAR(255)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#engine = create_engine(f\"mysql+mysqlconnector://{db_settings['user']}:{db_settings['password']}@{db_settings['host']}/{db_settings['database']}\")\n",
    "#df.compute().to_sql('uber_data', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "\n",
    "# Update your connection string to use an empty password\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{db_settings['user']}:{db_settings['password']}@{db_settings['host']}:{db_settings['port']}/{db_settings['database']}\")\n",
    "\n",
    "\n",
    "df.compute().to_sql('uber_data', con=engine, if_exists='replace', index=False)\n",
    "# Close the MySQL connection\n",
    "connection.close()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# MySQL database connection settings\n",
    "db_settings = {\n",
    "    'host': 'localhost',\n",
    "    'port': '3307',  # Use the IP or hostname of your MySQL container\n",
    "    'user': 'root',\n",
    "    'password': 'Wen2003',\n",
    "    'database': 'vinit',\n",
    "}\n",
    "\n",
    "# Your existing code to read and process CSV files using Dask\n",
    "base_mapping = {\n",
    "    'B02512': 'Unter',\n",
    "    'B02598': 'Hinter',\n",
    "    'B02617': 'Weiter',\n",
    "    'B02682': 'Schmecken',\n",
    "    'B02764': 'Danach-NY',\n",
    "    'B02765': 'Grun',\n",
    "    'B02835': 'Dreist',\n",
    "    'B02836': 'Drinnen'\n",
    "}\n",
    "\n",
    "file_paths = [\n",
    "    \"C://Users//jain vinit//Documents//FExam//uber-tlc-foil-response//uber-trip-data//uber-raw-data-aug14.csv\",\n",
    "    \"C://Users//jain vinit//Documents//FExam//uber-tlc-foil-response//uber-trip-data//uber-raw-data-jul14.csv\",\n",
    "\n",
    "]\n",
    "\n",
    "ddf_list = [dd.read_csv(file) for file in file_paths]\n",
    "df = dd.concat(ddf_list)\n",
    "\n",
    "# Additional processing, mapping, etc.\n",
    "df['Base'] = df['Base'].map(base_mapping.get, na_action='ignore')\n",
    "\n",
    "# MySQL connection using SQLAlchemy and connection pooling\n",
    "engine = create_engine(\n",
    "    f\"mysql+mysqlconnector://{db_settings['user']}:{db_settings['password']}@{db_settings['host']}:{db_settings['port']}/{db_settings['database']}\",\n",
    "    pool_size=10,  # Adjust pool size based on your needs\n",
    "    max_overflow=20\n",
    ")\n",
    "\n",
    "# Create the table if it doesn't exist\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_data (\n",
    "    Date DATE,\n",
    "    Lat FLOAT,\n",
    "    Lon FLOAT,\n",
    "    Base VARCHAR(255)\n",
    ");\n",
    "\"\"\"\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(create_table_query)\n",
    "\n",
    "# Load CSV data into MySQL table\n",
    "df.compute().to_sql('uber_data2', con=engine, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "\n",
    "# MySQL database connection settings\n",
    "db_settings = {\n",
    "    'host': 'localhost',\n",
    "    'port': '3307',  # Use the IP or hostname of your MySQL container\n",
    "    'user': 'root',\n",
    "    'password': 'Wen2003',\n",
    "    'database': 'vinit',\n",
    "}\n",
    "\n",
    "# Your existing code to read and process CSV files using Dask\n",
    "base_mapping = {\n",
    "    'B02512': 'Unter',\n",
    "    'B02598': 'Hinter',\n",
    "    'B02617': 'Weiter',\n",
    "    'B02682': 'Schmecken',\n",
    "    'B02764': 'Danach-NY',\n",
    "    'B02765': 'Grun',\n",
    "    'B02835': 'Dreist',\n",
    "    'B02836': 'Drinnen'\n",
    "}\n",
    "\n",
    "file_paths = [\n",
    "    \"C://Users//jain vinit//Documents//FExam//uber-tlc-foil-response//uber-trip-data//uber-raw-data-aug14.csv\",\n",
    "    \"C://Users//jain vinit//Documents//FExam//uber-tlc-foil-response//uber-trip-data//uber-raw-data-jul14.csv\",\n",
    "]\n",
    "\n",
    "# Create a Dask distributed client to enable parallel processing\n",
    "client = Client(dashboard_address=':8788')\n",
    "client = Client()\n",
    "\n",
    "# Read CSV files in parallel using Dask\n",
    "ddf_list = [dd.read_csv(file, assume_missing=True) for file in file_paths]\n",
    "\n",
    "# Additional processing, mapping, etc.\n",
    "for i, ddf in enumerate(ddf_list):\n",
    "    ddf_list[i] = ddf.map_partitions(lambda df: df.assign(Base=df['Base'].map(base_mapping)), meta=ddf)\n",
    "\n",
    "# Concatenate Dask DataFrames\n",
    "df = dd.concat(ddf_list)\n",
    "\n",
    "# MySQL connection using SQLAlchemy and connection pooling\n",
    "engine = create_engine(\n",
    "    f\"mysql+mysqlconnector://{db_settings['user']}:{db_settings['password']}@{db_settings['host']}:{db_settings['port']}/{db_settings['database']}\",\n",
    "    pool_size=10,  # Adjust pool size based on your needs\n",
    "    max_overflow=20\n",
    ")\n",
    "\n",
    "# Create the table if it doesn't exist\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_data (\n",
    "    Date DATE,\n",
    "    Lat FLOAT,\n",
    "    Lon FLOAT,\n",
    "    Base VARCHAR(255)\n",
    ");\n",
    "\"\"\"\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(create_table_query)\n",
    "\n",
    "# Load CSV data into MySQL table\n",
    "df.compute().to_sql('uber_data', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Close the Dask distributed client\n",
    "client.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
